{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DuckDB-Parquet.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/avnsiva/BANKNIFTYDATA/blob/master/DuckDB_Parquet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-kqtWblkoujV"
      },
      "source": [
        "## Setup\n",
        "First we download some files and install the required libraries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39OPDvzADNMz"
      },
      "source": [
        "!pip install pyarrow pandas\n",
        "!pip install duckdb --pre --upgrade\n",
        "!mkdir -p taxi\n",
        "!wget https://github.com/cwida/duckdb-data/releases/download/v1.0/taxi_2019_04.parquet -O taxi/201904.parquet\n",
        "!wget https://github.com/cwida/duckdb-data/releases/download/v1.0/taxi_2019_05.parquet -O taxi/201905.parquet\n",
        "!wget https://github.com/cwida/duckdb-data/releases/download/v1.0/taxi_2019_06.parquet -O taxi/201906.parquet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imYHieGpD7_e"
      },
      "source": [
        "import pyarrow.parquet as pq\n",
        "import pandas\n",
        "import glob\n",
        "import duckdb\n",
        "\n",
        "# some DuckDB setup \n",
        "con = duckdb.connect()\n",
        "# enable automatic query parallelization\n",
        "con.execute(\"PRAGMA threads=2\")\n",
        "# enable caching of parquet metadata\n",
        "con.execute(\"PRAGMA enable_object_cache\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGtaQW0L11LT"
      },
      "source": [
        "## Reading Multiple Parquet Files\n",
        "\n",
        "DuckDB can read multiple parquet files using the glob syntax.\n",
        "\n",
        "In Pandas, we need to load the files separately and concatenate them together into a single DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_Kl55nvhfPy"
      },
      "source": [
        "%%time\n",
        "con.execute(\"SELECT * FROM 'taxi/*.parquet' LIMIT 5\").df()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iudIWgW-e29W"
      },
      "source": [
        "%%time\n",
        "df = pandas.concat(\n",
        "\t[pandas.read_parquet(file)\n",
        "\t for file\n",
        "\t in glob.glob('taxi/*.parquet')])\n",
        "print(df.head(5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08pmu4IN2TPA"
      },
      "source": [
        "## Concatenate the three files into a single large file \n",
        "\n",
        "As Pandas does not have native support for reading multiple files, we perform the remaining experiments on a single large file.\n",
        "\n",
        "We use the pyarrow library to concatenate the three files into a single file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CISvtKWrfgJP"
      },
      "source": [
        "# concatenate all three parquet files into a single file\n",
        "pq.write_table(pq.ParquetDataset('taxi/').read(), 'alltaxi.parquet', row_group_size=100000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ER2RRs24Ikx"
      },
      "source": [
        "### Querying the Single File\n",
        "\n",
        "Now let's query the single file and check the achieved performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vujLPgUqHztl"
      },
      "source": [
        "%%time\n",
        "con.execute(\"SELECT * FROM 'alltaxi.parquet' LIMIT 5\").df()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6plLAHdN39y5"
      },
      "source": [
        "%%time\n",
        "pandas.read_parquet('alltaxi.parquet').head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQZyP6Er6B-8"
      },
      "source": [
        "## Counting the Rows\n",
        "\n",
        "Now suppose we want to figure out how many rows are in our data set. We can do that using the following code snippets.\n",
        "\n",
        "Note that by default Pandas will read the entire Parquet file into memory again. We can manually optimize the query by specifying that only a single column should be loaded.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7usVA6OSlsOE"
      },
      "source": [
        "%%time\n",
        "# DuckDB\n",
        "print(con.execute(\"SELECT COUNT(*) FROM 'alltaxi.parquet'\").df())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4-v5dL5H921"
      },
      "source": [
        "%%time\n",
        "# Pandas (naive)\n",
        "print(len(pandas.read_parquet('alltaxi.parquet')))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JwYp_WhvIkW0"
      },
      "source": [
        "%%time\n",
        "# Pandas (projection pushdown)\n",
        "print(len(pandas.read_parquet('alltaxi.parquet', columns=['vendor_id'])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frXCfU-u8p9R"
      },
      "source": [
        "# Filtering Rows\n",
        "It is common to use some sort of filtering predicate to only look at the interesting parts of a data set. For example, imagine we want to know how many taxi rides occur after the 30th of June 2019. We can do that using the following queries in both Pandas and DuckDB.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVQpHs4DJHAQ"
      },
      "source": [
        "%%time\n",
        "# DuckDB\n",
        "con.execute(\"SELECT COUNT(*) FROM 'alltaxi.parquet' WHERE pickup_at > '2019-06-30'\").df()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nH-nuAfCJTRr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30f2968e-9027-4a8a-9fca-11ef1a09acaa"
      },
      "source": [
        "%%time\n",
        "# Pandas (naive)\n",
        "len(pandas.read_parquet('alltaxi.parquet')\n",
        "          .query(\"pickup_at > '2019-06-30'\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 11.5 s, sys: 9.39 s, total: 20.9 s\n",
            "Wall time: 15.5 s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "167022"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUdGqKBTJutf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e872513-249d-4f9d-8da9-3b52e04ad846"
      },
      "source": [
        "%%time\n",
        "# Pandas (projection pushdown)\n",
        "len(pandas.read_parquet('alltaxi.parquet', columns=['pickup_at'])\n",
        "          .query(\"pickup_at > '2019-06-30'\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 624 ms, sys: 350 ms, total: 974 ms\n",
            "Wall time: 940 ms\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "167022"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQFlOTKCKGro"
      },
      "source": [
        "%%time\n",
        "# Pandas (projection + filter pushdown)\n",
        "len(pandas.read_parquet('alltaxi.parquet', columns=['pickup_at'], filters=[('pickup_at', '>', '2019-06-30')]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pDVawGeiCCdR"
      },
      "source": [
        "df = pandas.read_parquet('alltaxi.parquet')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "td7w1Pi-CGR8"
      },
      "source": [
        "%%time\n",
        "# Pandas native\n",
        "print(len(df[['pickup_at']].query(\"pickup_at > '2019-06-30'\")))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vfhR9FHDYrp"
      },
      "source": [
        "## Aggregates\n",
        "\n",
        "Now suppose we want to figure out how many rows are in our data set. We can do that using the following code snippets.\n",
        "\n",
        "Note that by default Pandas will read the entire Parquet file into memory again. We can manually optimize the query by specifying that only a single column should be loaded.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bPlEXfuIKbf2"
      },
      "source": [
        "%%time\n",
        "# DuckDB (SQL)\n",
        "con.execute(\"SELECT passenger_count, COUNT(*) FROM 'alltaxi.parquet' GROUP BY passenger_count\").df()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9MeOo-ADTRR"
      },
      "source": [
        "## "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKCWlp7VMrAF"
      },
      "source": [
        "%%time\n",
        "# DuckDB (relational API)\n",
        "con.from_parquet('alltaxi.parquet'\n",
        "     ).aggregate('passenger_count, count(*)').df()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xchbsEwoL5ll"
      },
      "source": [
        "%%time\n",
        "# Pandas (naive)\n",
        "pandas.read_parquet('alltaxi.parquet').groupby('passenger_count').agg({'passenger_count' : 'count'})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8WEZBEYMEja"
      },
      "source": [
        "%%time\n",
        "# Pandas (projection pushdown)\n",
        "pandas.read_parquet('alltaxi.parquet', columns=['passenger_count']).groupby('passenger_count').agg({'passenger_count' : 'count'})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTdx54ecw7x9"
      },
      "source": [
        "%%time\n",
        "# Pandas (native)\n",
        "df[['passenger_count']].groupby('passenger_count').agg({'passenger_count' : 'count'})"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}